{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 3 - Making RL PySC2 Agent with sparse reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Runnning 'Agent code' on jupyter notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unfortunately, PySC2 uses Abseil, which treats python code as if its run like an app\n",
    "# This does not play well with jupyter notebook\n",
    "# So we will need to monkeypatch sys.argv\n",
    "\n",
    "\n",
    "import sys\n",
    "#sys.argv = [\"python\", \"--map\", \"AbyssalReef\"]\n",
    "sys.argv = [\"python\", \"--map\", \"Simple64\"]\n",
    "\n",
    "# Copyright 2017 Google Inc. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS-IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\"Run an agent.\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import importlib\n",
    "import threading\n",
    "\n",
    "from absl import app\n",
    "from absl import flags\n",
    "from future.builtins import range  # pylint: disable=redefined-builtin\n",
    "\n",
    "from pysc2 import maps\n",
    "from pysc2.env import available_actions_printer\n",
    "from pysc2.env import run_loop\n",
    "from pysc2.env import sc2_env\n",
    "from pysc2.lib import point_flag\n",
    "from pysc2.lib import stopwatch\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "# because of Abseil's horrible design for running code underneath Colabs\n",
    "# We have to pull out this ugly hack from the hat\n",
    "if \"flags_defined\" not in globals():\n",
    "    flags.DEFINE_bool(\"render\", False, \"Whether to render with pygame.\")\n",
    "    point_flag.DEFINE_point(\"feature_screen_size\", \"84\",\n",
    "                            \"Resolution for screen feature layers.\")\n",
    "    point_flag.DEFINE_point(\"feature_minimap_size\", \"64\",\n",
    "                            \"Resolution for minimap feature layers.\")\n",
    "    point_flag.DEFINE_point(\"rgb_screen_size\", None,\n",
    "                            \"Resolution for rendered screen.\")\n",
    "    point_flag.DEFINE_point(\"rgb_minimap_size\", None,\n",
    "                            \"Resolution for rendered minimap.\")\n",
    "    flags.DEFINE_enum(\"action_space\", None, sc2_env.ActionSpace._member_names_,  # pylint: disable=protected-access\n",
    "                      \"Which action space to use. Needed if you take both feature \"\n",
    "                      \"and rgb observations.\")\n",
    "    flags.DEFINE_bool(\"use_feature_units\", True,\n",
    "                      \"Whether to include feature units.\")\n",
    "    flags.DEFINE_bool(\"disable_fog\", True, \"Whether to disable Fog of War.\")\n",
    "\n",
    "    flags.DEFINE_integer(\"max_agent_steps\", 0, \"Total agent steps.\")\n",
    "    flags.DEFINE_integer(\"game_steps_per_episode\", None, \"Game steps per episode.\")\n",
    "    flags.DEFINE_integer(\"max_episodes\", 0, \"Total episodes.\")\n",
    "    flags.DEFINE_integer(\"step_mul\", 8, \"Game steps per agent step.\")\n",
    "    flags.DEFINE_float(\"fps\", 22.4, \"Frames per second to run the game.\")\n",
    "\n",
    "    #flags.DEFINE_string(\"agent\", \"sc2.agent.BasicAgent.ZergBasicAgent\",\n",
    "    #                    \"Which agent to run, as a python path to an Agent class.\")\n",
    "    #flags.DEFINE_enum(\"agent_race\", \"zerg\", sc2_env.Race._member_names_,  # pylint: disable=protected-access\n",
    "    #                  \"Agent 1's race.\")\n",
    "    flags.DEFINE_string(\"agent\", \"TerranSparseRewardRLAgent\",\n",
    "                        \"Which agent to run, as a python path to an Agent class.\")\n",
    "    flags.DEFINE_enum(\"agent_race\", \"terran\", sc2_env.Race._member_names_,  # pylint: disable=protected-access\n",
    "                      \"Agent 1's race.\")\n",
    "\n",
    "    flags.DEFINE_string(\"agent2\", \"Bot\", \"Second agent, either Bot or agent class.\")\n",
    "    flags.DEFINE_enum(\"agent2_race\", \"terran\", sc2_env.Race._member_names_,  # pylint: disable=protected-access\n",
    "                      \"Agent 2's race.\")\n",
    "    flags.DEFINE_enum(\"difficulty\", \"very_easy\", sc2_env.Difficulty._member_names_,  # pylint: disable=protected-access\n",
    "                      \"If agent2 is a built-in Bot, it's strength.\")\n",
    "\n",
    "    flags.DEFINE_bool(\"profile\", False, \"Whether to turn on code profiling.\")\n",
    "    flags.DEFINE_bool(\"trace\", False, \"Whether to trace the code execution.\")\n",
    "    flags.DEFINE_integer(\"parallel\", 1, \"How many instances to run in parallel.\")\n",
    "\n",
    "    flags.DEFINE_bool(\"save_replay\", True, \"Whether to save a replay at the end.\")\n",
    "\n",
    "    flags.DEFINE_string(\"map\", None, \"Name of a map to use.\")\n",
    "    flags.mark_flag_as_required(\"map\")\n",
    "\n",
    "flags_defined = True\n",
    "\n",
    "def run_thread(agent_classes, players, map_name, visualize):\n",
    "  \"\"\"Run one thread worth of the environment with agents.\"\"\"\n",
    "  with sc2_env.SC2Env(\n",
    "      map_name=map_name,\n",
    "      players=players,\n",
    "      agent_interface_format=sc2_env.parse_agent_interface_format(\n",
    "          feature_screen=FLAGS.feature_screen_size,\n",
    "          feature_minimap=FLAGS.feature_minimap_size,\n",
    "          rgb_screen=FLAGS.rgb_screen_size,\n",
    "          rgb_minimap=FLAGS.rgb_minimap_size,\n",
    "          action_space=FLAGS.action_space,\n",
    "          use_feature_units=FLAGS.use_feature_units),\n",
    "      step_mul=FLAGS.step_mul,\n",
    "      game_steps_per_episode=FLAGS.game_steps_per_episode,\n",
    "      disable_fog=FLAGS.disable_fog,\n",
    "      visualize=visualize) as env:\n",
    "    env = available_actions_printer.AvailableActionsPrinter(env)\n",
    "    agents = [agent_cls() for agent_cls in agent_classes]\n",
    "    run_loop.run_loop(agents, env, FLAGS.max_agent_steps, FLAGS.max_episodes)\n",
    "    if FLAGS.save_replay:\n",
    "      env.save_replay(agent_classes[0].__name__)\n",
    "\n",
    "def main(unused_argv):\n",
    "  \"\"\"Run an agent.\"\"\"\n",
    "  #stopwatch.sw.enabled = FLAGS.profile or FLAGS.trace\n",
    "  #stopwatch.sw.trace = FLAGS.trace\n",
    "\n",
    "  map_inst = maps.get(FLAGS.map)\n",
    "\n",
    "  agent_classes = []\n",
    "  players = []\n",
    "\n",
    "  #agent_module, agent_name = FLAGS.agent.rsplit(\".\", 1)\n",
    "  #agent_cls = getattr(importlib.import_module(agent_module), agent_name)\n",
    "  #agent_classes.append(agent_cls)\n",
    "  agent_classes.append(TerranSparseRewardRLAgent)\n",
    "  players.append(sc2_env.Agent(sc2_env.Race[FLAGS.agent_race]))\n",
    "\n",
    "  if map_inst.players >= 2:\n",
    "    if FLAGS.agent2 == \"Bot\":\n",
    "      players.append(sc2_env.Bot(sc2_env.Race[FLAGS.agent2_race],\n",
    "                                 sc2_env.Difficulty[FLAGS.difficulty]))\n",
    "    else:\n",
    "      agent_module, agent_name = FLAGS.agent2.rsplit(\".\", 1)\n",
    "      agent_cls = getattr(importlib.import_module(agent_module), agent_name)\n",
    "      agent_classes.append(agent_cls)\n",
    "      players.append(sc2_env.Agent(sc2_env.Race[FLAGS.agent2_race]))\n",
    "\n",
    "  threads = []\n",
    "  for _ in range(FLAGS.parallel - 1):\n",
    "    t = threading.Thread(target=run_thread,\n",
    "                         args=(agent_classes, players, FLAGS.map, False))\n",
    "    threads.append(t)\n",
    "    t.start()\n",
    "\n",
    "  run_thread(agent_classes, players, FLAGS.map, FLAGS.render)\n",
    "\n",
    "  for t in threads:\n",
    "    t.join()\n",
    "\n",
    "  if FLAGS.profile:\n",
    "    pass\n",
    "    #print(stopwatch.sw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Creating a RL PySC2 Agent with Sparse Reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import math\n",
    "import os.path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from pysc2.agents import base_agent\n",
    "from pysc2.env import sc2_env\n",
    "from pysc2.lib import actions, features, units\n",
    "from absl import app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FILE = 'rlagent_with_sparse_reward_learning_data'\n",
    "\n",
    "ACTION_DO_NOTHING = 'donothing'\n",
    "ACTION_BUILD_SUPPLY_DEPOT = 'buildsupplydepot'\n",
    "ACTION_BUILD_BARRACKS = 'buildbarracks'\n",
    "ACTION_BUILD_MARINE = 'buildmarine'\n",
    "ACTION_ATTACK = 'attack'\n",
    "\n",
    "smart_actions = [\n",
    "    ACTION_DO_NOTHING,\n",
    "    ACTION_BUILD_SUPPLY_DEPOT,\n",
    "    ACTION_BUILD_BARRACKS,\n",
    "    ACTION_BUILD_MARINE,\n",
    "]\n",
    "\n",
    "for mm_x in range(0, 64):\n",
    "    for mm_y in range(0, 64):\n",
    "        if (mm_x + 1) % 32 == 0 and (mm_y + 1) % 32 == 0:\n",
    "            smart_actions.append(ACTION_ATTACK + '_' + str(mm_x - 16) + '_' + str(mm_y - 16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference from https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow\n",
    "class QLearningTable:\n",
    "    def __init__(self, actions, learning_rate=0.01, reward_decay=0.9, e_greedy=0.9):\n",
    "        self.actions = actions  # a list\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = reward_decay\n",
    "        self.epsilon = e_greedy\n",
    "        self.q_table = pd.DataFrame(columns=self.actions, dtype=np.float64)\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        self.check_state_exist(observation)\n",
    "        \n",
    "        if np.random.uniform() < self.epsilon:\n",
    "            # choose best action\n",
    "            #state_action = self.q_table.ix[observation, :]\n",
    "            state_action = self.q_table.loc[observation, self.q_table.columns[:]]\n",
    "            \n",
    "            # some actions have the same value\n",
    "            state_action = state_action.reindex(np.random.permutation(state_action.index))\n",
    "            \n",
    "            action = state_action.idxmax()\n",
    "        else:\n",
    "            # choose random action\n",
    "            action = np.random.choice(self.actions)\n",
    "            \n",
    "        return action\n",
    "\n",
    "    def learn(self, s, a, r, s_):\n",
    "        self.check_state_exist(s_)\n",
    "        self.check_state_exist(s)\n",
    "        \n",
    "        #q_predict = self.q_table.ix[s, a]\n",
    "        q_predict = self.q_table.loc[s, a]\n",
    "        \n",
    "        if s_ != 'terminal':\n",
    "            #q_target = r + self.gamma * self.q_table.ix[s_, :].max()\n",
    "            q_target = r + self.gamma * self.q_table.loc[s_, self.q_table.columns[:]].max()\n",
    "        else:\n",
    "            q_target = r  # next state is terminal\n",
    "            \n",
    "        # update\n",
    "        #self.q_table.ix[s, a] += self.lr * (q_target - q_predict)\n",
    "        self.q_table.loc[s, a] += self.lr * (q_target - q_predict)\n",
    "\n",
    "    def check_state_exist(self, state):\n",
    "        if state not in self.q_table.index:\n",
    "            # append new state to q table\n",
    "            self.q_table = self.q_table.append(pd.Series([0] * len(self.actions), index=self.q_table.columns, name=state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TerranSparseRewardRLAgent(base_agent.BaseAgent):\n",
    "    def __init__(self):\n",
    "        super(TerranSparseRewardRLAgent, self).__init__()\n",
    "        \n",
    "        self.qlearn = QLearningTable(actions=list(range(len(smart_actions))))\n",
    "        \n",
    "        self.previous_action = None\n",
    "        self.previous_state = None\n",
    "        \n",
    "        self.cc_y = None\n",
    "        self.cc_x = None\n",
    "        \n",
    "        self.move_number = 0\n",
    "        \n",
    "        if os.path.isfile(DATA_FILE + '.gz'):\n",
    "            self.qlearn.q_table = pd.read_pickle(DATA_FILE + '.gz', compression='gzip')\n",
    "\n",
    "    def transformDistance(self, x, x_distance, y, y_distance):\n",
    "        if not self.base_top_left:\n",
    "            return [x - x_distance, y - y_distance]\n",
    "        \n",
    "        return [x + x_distance, y + y_distance]\n",
    "    \n",
    "    def transformLocation(self, x, y):\n",
    "        if not self.base_top_left:\n",
    "            return [64 - x, 64 - y]\n",
    "        \n",
    "        return [x, y]\n",
    "    \n",
    "    def getMeanLocation(self, unitList):\n",
    "        sum_x = 0\n",
    "        sum_y = 0\n",
    "        for unit in unitList:\n",
    "            sum_x += unit.x\n",
    "            sum_y += unit.y\n",
    "        mean_x = sum_x / len(unitList)\n",
    "        mean_y = sum_y / len(unitList)\n",
    "        \n",
    "        return [mean_x, mean_y]\n",
    "    \n",
    "    def splitAction(self, action_id):\n",
    "        smart_action = smart_actions[action_id]\n",
    "            \n",
    "        x = 0\n",
    "        y = 0\n",
    "        if '_' in smart_action:\n",
    "            smart_action, x, y = smart_action.split('_')\n",
    "\n",
    "        return (smart_action, x, y)\n",
    "    \n",
    "    def unit_type_is_selected(self, obs, unit_type):\n",
    "        if (len(obs.observation.single_select) > 0 and\n",
    "            obs.observation.single_select[0].unit_type == unit_type):\n",
    "              return True\n",
    "\n",
    "        if (len(obs.observation.multi_select) > 0 and\n",
    "            obs.observation.multi_select[0].unit_type == unit_type):\n",
    "              return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def get_units_by_type(self, obs, unit_type):\n",
    "        return [unit for unit in obs.observation.feature_units\n",
    "                if unit.unit_type == unit_type]\n",
    "\n",
    "    def can_do(self, obs, action):\n",
    "        return action in obs.observation.available_actions\n",
    "        \n",
    "    def step(self, obs):\n",
    "        super(TerranSparseRewardRLAgent, self).step(obs)\n",
    "\n",
    "        #time.sleep(0.5)\n",
    "        \n",
    "        if obs.first():\n",
    "            player_y, player_x = (obs.observation.feature_minimap.player_relative == features.PlayerRelative.SELF).nonzero()\n",
    "            self.base_top_left = 1 if player_y.any() and player_y.mean() <= 31 else 0\n",
    "            print(\"player_y: \", player_y)\n",
    "            print(\"player_y.mean(): \", player_y.mean())\n",
    "            print(\"base_top_left: \", self.base_top_left)\n",
    "            print(\"smart_actions: \", smart_actions)\n",
    "        \n",
    "        ccs = self.get_units_by_type(obs, units.Terran.CommandCenter)\n",
    "        if len(ccs) > 0:\n",
    "            self.cc_x, self.cc_y = self.getMeanLocation(ccs)\n",
    "            \n",
    "        cc_count = len(ccs)\n",
    "        \n",
    "        supply_depot_count = len(self.get_units_by_type(obs, units.Terran.SupplyDepot))\n",
    "\n",
    "        barracks_count = len(self.get_units_by_type(obs, units.Terran.Barracks))\n",
    "            \n",
    "        return actions.FUNCTIONS.no_op()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [run code]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "  app.run(main)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Adding 1st Step of Hierarchy Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import math\n",
    "import os.path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from pysc2.agents import base_agent\n",
    "from pysc2.env import sc2_env\n",
    "from pysc2.lib import actions, features, units\n",
    "from absl import app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FILE = 'rlagent_with_sparse_reward_learning_data'\n",
    "\n",
    "ACTION_DO_NOTHING = 'donothing'\n",
    "ACTION_BUILD_SUPPLY_DEPOT = 'buildsupplydepot'\n",
    "ACTION_BUILD_BARRACKS = 'buildbarracks'\n",
    "ACTION_BUILD_MARINE = 'buildmarine'\n",
    "ACTION_ATTACK = 'attack'\n",
    "\n",
    "smart_actions = [\n",
    "    ACTION_DO_NOTHING,\n",
    "    ACTION_BUILD_SUPPLY_DEPOT,\n",
    "    ACTION_BUILD_BARRACKS,\n",
    "    ACTION_BUILD_MARINE,\n",
    "]\n",
    "\n",
    "for mm_x in range(0, 64):\n",
    "    for mm_y in range(0, 64):\n",
    "        if (mm_x + 1) % 32 == 0 and (mm_y + 1) % 32 == 0:\n",
    "            smart_actions.append(ACTION_ATTACK + '_' + str(mm_x - 16) + '_' + str(mm_y - 16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference from https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow\n",
    "class QLearningTable:\n",
    "    def __init__(self, actions, learning_rate=0.01, reward_decay=0.9, e_greedy=0.9):\n",
    "        self.actions = actions  # a list\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = reward_decay\n",
    "        self.epsilon = e_greedy\n",
    "        self.q_table = pd.DataFrame(columns=self.actions, dtype=np.float64)\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        self.check_state_exist(observation)\n",
    "        \n",
    "        if np.random.uniform() < self.epsilon:\n",
    "            # choose best action\n",
    "            #state_action = self.q_table.ix[observation, :]\n",
    "            state_action = self.q_table.loc[observation, self.q_table.columns[:]]\n",
    "            \n",
    "            # some actions have the same value\n",
    "            state_action = state_action.reindex(np.random.permutation(state_action.index))\n",
    "            \n",
    "            action = state_action.idxmax()\n",
    "        else:\n",
    "            # choose random action\n",
    "            action = np.random.choice(self.actions)\n",
    "            \n",
    "        return action\n",
    "\n",
    "    def learn(self, s, a, r, s_):\n",
    "        self.check_state_exist(s_)\n",
    "        self.check_state_exist(s)\n",
    "        \n",
    "        #q_predict = self.q_table.ix[s, a]\n",
    "        q_predict = self.q_table.loc[s, a]\n",
    "        \n",
    "        if s_ != 'terminal':\n",
    "            #q_target = r + self.gamma * self.q_table.ix[s_, :].max()\n",
    "            q_target = r + self.gamma * self.q_table.loc[s_, self.q_table.columns[:]].max()\n",
    "        else:\n",
    "            q_target = r  # next state is terminal\n",
    "            \n",
    "        # update\n",
    "        #self.q_table.ix[s, a] += self.lr * (q_target - q_predict)\n",
    "        self.q_table.loc[s, a] += self.lr * (q_target - q_predict)\n",
    "\n",
    "    def check_state_exist(self, state):\n",
    "        if state not in self.q_table.index:\n",
    "            # append new state to q table\n",
    "            self.q_table = self.q_table.append(pd.Series([0] * len(self.actions), index=self.q_table.columns, name=state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TerranSparseRewardRLAgent(base_agent.BaseAgent):\n",
    "    def __init__(self):\n",
    "        super(TerranSparseRewardRLAgent, self).__init__()\n",
    "        \n",
    "        self.qlearn = QLearningTable(actions=list(range(len(smart_actions))))\n",
    "        \n",
    "        self.previous_action = None\n",
    "        self.previous_state = None\n",
    "        \n",
    "        self.cc_y = None\n",
    "        self.cc_x = None\n",
    "        \n",
    "        self.move_number = 0\n",
    "        \n",
    "        if os.path.isfile(DATA_FILE + '.gz'):\n",
    "            self.qlearn.q_table = pd.read_pickle(DATA_FILE + '.gz', compression='gzip')\n",
    "\n",
    "    def transformDistance(self, x, x_distance, y, y_distance):\n",
    "        if not self.base_top_left:\n",
    "            return [x - x_distance, y - y_distance]\n",
    "        \n",
    "        return [x + x_distance, y + y_distance]\n",
    "    \n",
    "    def transformLocation(self, x, y):\n",
    "        if not self.base_top_left:\n",
    "            return [64 - x, 64 - y]\n",
    "        \n",
    "        return [x, y]\n",
    "    \n",
    "    def getMeanLocation(self, unitList):\n",
    "        sum_x = 0\n",
    "        sum_y = 0\n",
    "        for unit in unitList:\n",
    "            sum_x += unit.x\n",
    "            sum_y += unit.y\n",
    "        mean_x = sum_x / len(unitList)\n",
    "        mean_y = sum_y / len(unitList)\n",
    "        \n",
    "        return [mean_x, mean_y]\n",
    "    \n",
    "    def splitAction(self, action_id):\n",
    "        smart_action = smart_actions[action_id]\n",
    "            \n",
    "        x = 0\n",
    "        y = 0\n",
    "        if '_' in smart_action:\n",
    "            smart_action, x, y = smart_action.split('_')\n",
    "\n",
    "        return (smart_action, x, y)\n",
    "    \n",
    "    def unit_type_is_selected(self, obs, unit_type):\n",
    "        if (len(obs.observation.single_select) > 0 and\n",
    "            obs.observation.single_select[0].unit_type == unit_type):\n",
    "              return True\n",
    "\n",
    "        if (len(obs.observation.multi_select) > 0 and\n",
    "            obs.observation.multi_select[0].unit_type == unit_type):\n",
    "              return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def get_units_by_type(self, obs, unit_type):\n",
    "        return [unit for unit in obs.observation.feature_units\n",
    "                if unit.unit_type == unit_type]\n",
    "\n",
    "    def can_do(self, obs, action):\n",
    "        return action in obs.observation.available_actions\n",
    "        \n",
    "    def step(self, obs):\n",
    "        super(TerranSparseRewardRLAgent, self).step(obs)\n",
    "\n",
    "        #time.sleep(0.5)\n",
    "        \n",
    "        if obs.first():\n",
    "            player_y, player_x = (obs.observation.feature_minimap.player_relative == features.PlayerRelative.SELF).nonzero()\n",
    "            self.base_top_left = 1 if player_y.any() and player_y.mean() <= 31 else 0\n",
    "        \n",
    "        ccs = self.get_units_by_type(obs, units.Terran.CommandCenter)\n",
    "        if len(ccs) > 0:\n",
    "            self.cc_x, self.cc_y = self.getMeanLocation(ccs)\n",
    "            \n",
    "        cc_count = len(ccs)\n",
    "        \n",
    "        supply_depot_count = len(self.get_units_by_type(obs, units.Terran.SupplyDepot))\n",
    "\n",
    "        barracks_count = len(self.get_units_by_type(obs, units.Terran.Barracks))\n",
    "\n",
    "        army_supply = obs.observation.player.food_used\n",
    "        \n",
    "        if self.move_number == 0:\n",
    "            self.move_number += 1\n",
    "            \n",
    "            current_state = np.zeros(8)\n",
    "            current_state[0] = cc_count\n",
    "            current_state[1] = supply_depot_count\n",
    "            current_state[2] = barracks_count\n",
    "            current_state[3] = army_supply\n",
    "    \n",
    "            hot_squares = np.zeros(4)        \n",
    "            enemy_y, enemy_x = (obs.observation.feature_minimap.player_relative == features.PlayerRelative.ENEMY).nonzero()\n",
    "            for i in range(0, len(enemy_y)):\n",
    "                y = int(math.ceil((enemy_y[i] + 1) / 32))\n",
    "                x = int(math.ceil((enemy_x[i] + 1) / 32))\n",
    "                \n",
    "                hot_squares[((y - 1) * 2) + (x - 1)] = 1\n",
    "            \n",
    "            if not self.base_top_left:\n",
    "                hot_squares = hot_squares[::-1]\n",
    "            \n",
    "            for i in range(0, 4):\n",
    "                current_state[i + 4] = hot_squares[i]\n",
    "    \n",
    "            if self.previous_action is not None:\n",
    "                self.qlearn.learn(str(self.previous_state), self.previous_action, 0, str(current_state))\n",
    "        \n",
    "            rl_action = self.qlearn.choose_action(str(current_state))\n",
    "\n",
    "            self.previous_state = current_state\n",
    "            self.previous_action = rl_action\n",
    "        \n",
    "            smart_action, x, y = self.splitAction(self.previous_action)\n",
    "            \n",
    "            if smart_action == ACTION_BUILD_BARRACKS or smart_action == ACTION_BUILD_SUPPLY_DEPOT:\n",
    "                if self.can_do(obs, actions.FUNCTIONS.select_point.id):\n",
    "                    scvs = self.get_units_by_type(obs, units.Terran.SCV)\n",
    "                    if len(scvs) > 0:\n",
    "                        scv = random.choice(scvs)\n",
    "                        if scv.x >= 0 and scv.y >= 0:\n",
    "                            return actions.FUNCTIONS.select_point(\"select\", (scv.x,\n",
    "                                                                              scv.y))\n",
    "                \n",
    "            elif smart_action == ACTION_BUILD_MARINE:\n",
    "                if self.can_do(obs, actions.FUNCTIONS.select_point.id):\n",
    "                    barracks = self.get_units_by_type(obs, units.Terran.Barracks)\n",
    "                    if len(barracks) > 0:\n",
    "                        barrack = random.choice(barracks)\n",
    "                        if barrack.x >= 0 and barrack.y >= 0:\n",
    "                            return actions.FUNCTIONS.select_point(\"select_all_type\", (barrack.x,\n",
    "                                                                                  barrack.y))\n",
    "                \n",
    "            elif smart_action == ACTION_ATTACK:\n",
    "                if self.can_do(obs, actions.FUNCTIONS.select_army.id):\n",
    "                    return actions.FUNCTIONS.select_army(\"select\")\n",
    "        \n",
    "        return actions.FUNCTIONS.no_op()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [run code]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "  app.run(main)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Adding 2nd Step of Hierarchy Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import math\n",
    "import os.path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from pysc2.agents import base_agent\n",
    "from pysc2.env import sc2_env\n",
    "from pysc2.lib import actions, features, units\n",
    "from absl import app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FILE = 'rlagent_with_sparse_reward_learning_data'\n",
    "\n",
    "ACTION_DO_NOTHING = 'donothing'\n",
    "ACTION_BUILD_SUPPLY_DEPOT = 'buildsupplydepot'\n",
    "ACTION_BUILD_BARRACKS = 'buildbarracks'\n",
    "ACTION_BUILD_MARINE = 'buildmarine'\n",
    "ACTION_ATTACK = 'attack'\n",
    "\n",
    "smart_actions = [\n",
    "    ACTION_DO_NOTHING,\n",
    "    ACTION_BUILD_SUPPLY_DEPOT,\n",
    "    ACTION_BUILD_BARRACKS,\n",
    "    ACTION_BUILD_MARINE,\n",
    "]\n",
    "\n",
    "for mm_x in range(0, 64):\n",
    "    for mm_y in range(0, 64):\n",
    "        if (mm_x + 1) % 32 == 0 and (mm_y + 1) % 32 == 0:\n",
    "            smart_actions.append(ACTION_ATTACK + '_' + str(mm_x - 16) + '_' + str(mm_y - 16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference from https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow\n",
    "class QLearningTable:\n",
    "    def __init__(self, actions, learning_rate=0.01, reward_decay=0.9, e_greedy=0.9):\n",
    "        self.actions = actions  # a list\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = reward_decay\n",
    "        self.epsilon = e_greedy\n",
    "        self.q_table = pd.DataFrame(columns=self.actions, dtype=np.float64)\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        self.check_state_exist(observation)\n",
    "        \n",
    "        if np.random.uniform() < self.epsilon:\n",
    "            # choose best action\n",
    "            #state_action = self.q_table.ix[observation, :]\n",
    "            state_action = self.q_table.loc[observation, self.q_table.columns[:]]\n",
    "            \n",
    "            # some actions have the same value\n",
    "            state_action = state_action.reindex(np.random.permutation(state_action.index))\n",
    "            \n",
    "            action = state_action.idxmax()\n",
    "        else:\n",
    "            # choose random action\n",
    "            action = np.random.choice(self.actions)\n",
    "            \n",
    "        return action\n",
    "\n",
    "    def learn(self, s, a, r, s_):\n",
    "        self.check_state_exist(s_)\n",
    "        self.check_state_exist(s)\n",
    "        \n",
    "        #q_predict = self.q_table.ix[s, a]\n",
    "        q_predict = self.q_table.loc[s, a]\n",
    "        \n",
    "        if s_ != 'terminal':\n",
    "            #q_target = r + self.gamma * self.q_table.ix[s_, :].max()\n",
    "            q_target = r + self.gamma * self.q_table.loc[s_, self.q_table.columns[:]].max()\n",
    "        else:\n",
    "            q_target = r  # next state is terminal\n",
    "            \n",
    "        # update\n",
    "        #self.q_table.ix[s, a] += self.lr * (q_target - q_predict)\n",
    "        self.q_table.loc[s, a] += self.lr * (q_target - q_predict)\n",
    "\n",
    "    def check_state_exist(self, state):\n",
    "        if state not in self.q_table.index:\n",
    "            # append new state to q table\n",
    "            self.q_table = self.q_table.append(pd.Series([0] * len(self.actions), index=self.q_table.columns, name=state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TerranSparseRewardRLAgent(base_agent.BaseAgent):\n",
    "    def __init__(self):\n",
    "        super(TerranSparseRewardRLAgent, self).__init__()\n",
    "        \n",
    "        self.qlearn = QLearningTable(actions=list(range(len(smart_actions))))\n",
    "        \n",
    "        self.previous_action = None\n",
    "        self.previous_state = None\n",
    "        \n",
    "        self.cc_y = None\n",
    "        self.cc_x = None\n",
    "        \n",
    "        self.move_number = 0\n",
    "        \n",
    "        if os.path.isfile(DATA_FILE + '.gz'):\n",
    "            self.qlearn.q_table = pd.read_pickle(DATA_FILE + '.gz', compression='gzip')\n",
    "\n",
    "    def transformDistance(self, x, x_distance, y, y_distance):\n",
    "        if not self.base_top_left:\n",
    "            return [x - x_distance, y - y_distance]\n",
    "        \n",
    "        return [x + x_distance, y + y_distance]\n",
    "    \n",
    "    def transformLocation(self, x, y):\n",
    "        if not self.base_top_left:\n",
    "            return [64 - x, 64 - y]\n",
    "        \n",
    "        return [x, y]\n",
    "    \n",
    "    def getMeanLocation(self, unitList):\n",
    "        sum_x = 0\n",
    "        sum_y = 0\n",
    "        for unit in unitList:\n",
    "            sum_x += unit.x\n",
    "            sum_y += unit.y\n",
    "        mean_x = sum_x / len(unitList)\n",
    "        mean_y = sum_y / len(unitList)\n",
    "        \n",
    "        return [mean_x, mean_y]\n",
    "    \n",
    "    def splitAction(self, action_id):\n",
    "        smart_action = smart_actions[action_id]\n",
    "            \n",
    "        x = 0\n",
    "        y = 0\n",
    "        if '_' in smart_action:\n",
    "            smart_action, x, y = smart_action.split('_')\n",
    "\n",
    "        return (smart_action, x, y)\n",
    "    \n",
    "    def unit_type_is_selected(self, obs, unit_type):\n",
    "        if (len(obs.observation.single_select) > 0 and\n",
    "            obs.observation.single_select[0].unit_type == unit_type):\n",
    "              return True\n",
    "\n",
    "        if (len(obs.observation.multi_select) > 0 and\n",
    "            obs.observation.multi_select[0].unit_type == unit_type):\n",
    "              return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def get_units_by_type(self, obs, unit_type):\n",
    "        return [unit for unit in obs.observation.feature_units\n",
    "                if unit.unit_type == unit_type]\n",
    "\n",
    "    def can_do(self, obs, action):\n",
    "        return action in obs.observation.available_actions\n",
    "        \n",
    "    def step(self, obs):\n",
    "        super(TerranSparseRewardRLAgent, self).step(obs)\n",
    "\n",
    "        #time.sleep(0.5)\n",
    "        \n",
    "        if obs.first():\n",
    "            player_y, player_x = (obs.observation.feature_minimap.player_relative == features.PlayerRelative.SELF).nonzero()\n",
    "            self.base_top_left = 1 if player_y.any() and player_y.mean() <= 31 else 0\n",
    "        \n",
    "        ccs = self.get_units_by_type(obs, units.Terran.CommandCenter)\n",
    "        if len(ccs) > 0:\n",
    "            self.cc_x, self.cc_y = self.getMeanLocation(ccs)\n",
    "            \n",
    "        cc_count = len(ccs)\n",
    "        \n",
    "        supply_depot_count = len(self.get_units_by_type(obs, units.Terran.SupplyDepot))\n",
    "\n",
    "        barracks_count = len(self.get_units_by_type(obs, units.Terran.Barracks))\n",
    "\n",
    "        army_supply = obs.observation.player.food_used\n",
    "        \n",
    "        if self.move_number == 0:\n",
    "            self.move_number += 1\n",
    "            \n",
    "            current_state = np.zeros(8)\n",
    "            current_state[0] = cc_count\n",
    "            current_state[1] = supply_depot_count\n",
    "            current_state[2] = barracks_count\n",
    "            current_state[3] = army_supply\n",
    "    \n",
    "            hot_squares = np.zeros(4)        \n",
    "            enemy_y, enemy_x = (obs.observation.feature_minimap.player_relative == features.PlayerRelative.ENEMY).nonzero()\n",
    "            for i in range(0, len(enemy_y)):\n",
    "                y = int(math.ceil((enemy_y[i] + 1) / 32))\n",
    "                x = int(math.ceil((enemy_x[i] + 1) / 32))\n",
    "                \n",
    "                hot_squares[((y - 1) * 2) + (x - 1)] = 1\n",
    "            \n",
    "            if not self.base_top_left:\n",
    "                hot_squares = hot_squares[::-1]\n",
    "            \n",
    "            for i in range(0, 4):\n",
    "                current_state[i + 4] = hot_squares[i]\n",
    "    \n",
    "            if self.previous_action is not None:\n",
    "                self.qlearn.learn(str(self.previous_state), self.previous_action, 0, str(current_state))\n",
    "        \n",
    "            rl_action = self.qlearn.choose_action(str(current_state))\n",
    "\n",
    "            self.previous_state = current_state\n",
    "            self.previous_action = rl_action\n",
    "        \n",
    "            smart_action, x, y = self.splitAction(self.previous_action)\n",
    "            \n",
    "            if smart_action == ACTION_BUILD_BARRACKS or smart_action == ACTION_BUILD_SUPPLY_DEPOT:\n",
    "                if self.can_do(obs, actions.FUNCTIONS.select_point.id):\n",
    "                    scvs = self.get_units_by_type(obs, units.Terran.SCV)\n",
    "                    if len(scvs) > 0:\n",
    "                        scv = random.choice(scvs)\n",
    "                        if scv.x >= 0 and scv.y >= 0:\n",
    "                            return actions.FUNCTIONS.select_point(\"select\", (scv.x,\n",
    "                                                                              scv.y))\n",
    "                \n",
    "            elif smart_action == ACTION_BUILD_MARINE:\n",
    "                if self.can_do(obs, actions.FUNCTIONS.select_point.id):\n",
    "                    barracks = self.get_units_by_type(obs, units.Terran.Barracks)\n",
    "                    if len(barracks) > 0:\n",
    "                        barrack = random.choice(barracks)\n",
    "                        if barrack.x >= 0 and barrack.y >= 0:\n",
    "                            return actions.FUNCTIONS.select_point(\"select_all_type\", (barrack.x,\n",
    "                                                                                  barrack.y))\n",
    "                \n",
    "            elif smart_action == ACTION_ATTACK:\n",
    "                if self.can_do(obs, actions.FUNCTIONS.select_army.id):\n",
    "                    return actions.FUNCTIONS.select_army(\"select\")\n",
    "                \n",
    "        elif self.move_number == 1:\n",
    "            self.move_number += 1\n",
    "            \n",
    "            smart_action, x, y = self.splitAction(self.previous_action)\n",
    "                \n",
    "            if smart_action == ACTION_BUILD_SUPPLY_DEPOT:\n",
    "                if supply_depot_count < 2 and self.can_do(obs, actions.FUNCTIONS.Build_SupplyDepot_screen.id):\n",
    "                    if len(ccs) > 0:\n",
    "                        if supply_depot_count == 0:\n",
    "                            target = self.transformDistance(self.cc_x, -35, self.cc_y, 0)\n",
    "                        elif supply_depot_count == 1:\n",
    "                            target = self.transformDistance(self.cc_x, -25, self.cc_y, -25)\n",
    "    \n",
    "                        return actions.FUNCTIONS.Build_SupplyDepot_screen(\"now\", target) \n",
    "            \n",
    "            elif smart_action == ACTION_BUILD_BARRACKS:\n",
    "                if barracks_count < 2 and self.can_do(obs, actions.FUNCTIONS.Build_Barracks_screen.id):\n",
    "                    if len(ccs) > 0:\n",
    "                        if  barracks_count == 0:\n",
    "                            target = self.transformDistance(self.cc_x, 15, self.cc_y, -9)\n",
    "                        elif  barracks_count == 1:\n",
    "                            target = self.transformDistance(self.cc_x, 15, self.cc_y, 12)\n",
    "    \n",
    "                        return actions.FUNCTIONS.Build_Barracks_screen(\"now\", target)\n",
    "    \n",
    "            elif smart_action == ACTION_BUILD_MARINE:\n",
    "                if self.can_do(obs, actions.FUNCTIONS.Train_Marine_quick.id):\n",
    "                    return actions.FUNCTIONS.Train_Marine_quick(\"queued\")\n",
    "        \n",
    "            elif smart_action == ACTION_ATTACK:\n",
    "                if self.can_do(obs, actions.FUNCTIONS.Attack_minimap.id):\n",
    "                    x_offset = random.randint(-1, 1)\n",
    "                    y_offset = random.randint(-1, 1)\n",
    "                    \n",
    "                    return actions.FUNCTIONS.Attack_minimap(\"now\", self.transformLocation(int(x) + (x_offset * 8), int(y) + (y_offset * 8)))\n",
    "            \n",
    "        return actions.FUNCTIONS.no_op()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [run code]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "  app.run(main)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Adding 3rd Step of Hierarchy Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import math\n",
    "import os.path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from pysc2.agents import base_agent\n",
    "from pysc2.env import sc2_env\n",
    "from pysc2.lib import actions, features, units\n",
    "from absl import app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FILE = 'rlagent_with_sparse_reward_learning_data'\n",
    "\n",
    "ACTION_DO_NOTHING = 'donothing'\n",
    "ACTION_BUILD_SUPPLY_DEPOT = 'buildsupplydepot'\n",
    "ACTION_BUILD_BARRACKS = 'buildbarracks'\n",
    "ACTION_BUILD_MARINE = 'buildmarine'\n",
    "ACTION_ATTACK = 'attack'\n",
    "\n",
    "smart_actions = [\n",
    "    ACTION_DO_NOTHING,\n",
    "    ACTION_BUILD_SUPPLY_DEPOT,\n",
    "    ACTION_BUILD_BARRACKS,\n",
    "    ACTION_BUILD_MARINE,\n",
    "]\n",
    "\n",
    "for mm_x in range(0, 64):\n",
    "    for mm_y in range(0, 64):\n",
    "        if (mm_x + 1) % 32 == 0 and (mm_y + 1) % 32 == 0:\n",
    "            smart_actions.append(ACTION_ATTACK + '_' + str(mm_x - 16) + '_' + str(mm_y - 16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference from https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow\n",
    "class QLearningTable:\n",
    "    def __init__(self, actions, learning_rate=0.01, reward_decay=0.9, e_greedy=0.9):\n",
    "        self.actions = actions  # a list\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = reward_decay\n",
    "        self.epsilon = e_greedy\n",
    "        self.q_table = pd.DataFrame(columns=self.actions, dtype=np.float64)\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        self.check_state_exist(observation)\n",
    "        \n",
    "        if np.random.uniform() < self.epsilon:\n",
    "            # choose best action\n",
    "            #state_action = self.q_table.ix[observation, :]\n",
    "            state_action = self.q_table.loc[observation, self.q_table.columns[:]]\n",
    "            \n",
    "            # some actions have the same value\n",
    "            state_action = state_action.reindex(np.random.permutation(state_action.index))\n",
    "            \n",
    "            action = state_action.idxmax()\n",
    "        else:\n",
    "            # choose random action\n",
    "            action = np.random.choice(self.actions)\n",
    "            \n",
    "        return action\n",
    "\n",
    "    def learn(self, s, a, r, s_):\n",
    "        self.check_state_exist(s_)\n",
    "        self.check_state_exist(s)\n",
    "        \n",
    "        #q_predict = self.q_table.ix[s, a]\n",
    "        q_predict = self.q_table.loc[s, a]\n",
    "        \n",
    "        if s_ != 'terminal':\n",
    "            #q_target = r + self.gamma * self.q_table.ix[s_, :].max()\n",
    "            q_target = r + self.gamma * self.q_table.loc[s_, self.q_table.columns[:]].max()\n",
    "        else:\n",
    "            q_target = r  # next state is terminal\n",
    "            \n",
    "        # update\n",
    "        #self.q_table.ix[s, a] += self.lr * (q_target - q_predict)\n",
    "        self.q_table.loc[s, a] += self.lr * (q_target - q_predict)\n",
    "\n",
    "    def check_state_exist(self, state):\n",
    "        if state not in self.q_table.index:\n",
    "            # append new state to q table\n",
    "            self.q_table = self.q_table.append(pd.Series([0] * len(self.actions), index=self.q_table.columns, name=state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TerranSparseRewardRLAgent(base_agent.BaseAgent):\n",
    "    def __init__(self):\n",
    "        super(TerranSparseRewardRLAgent, self).__init__()\n",
    "        \n",
    "        self.qlearn = QLearningTable(actions=list(range(len(smart_actions))))\n",
    "        \n",
    "        self.previous_action = None\n",
    "        self.previous_state = None\n",
    "        \n",
    "        self.cc_y = None\n",
    "        self.cc_x = None\n",
    "        \n",
    "        self.move_number = 0\n",
    "        \n",
    "        if os.path.isfile(DATA_FILE + '.gz'):\n",
    "            self.qlearn.q_table = pd.read_pickle(DATA_FILE + '.gz', compression='gzip')\n",
    "\n",
    "    def transformDistance(self, x, x_distance, y, y_distance):\n",
    "        if not self.base_top_left:\n",
    "            return [x - x_distance, y - y_distance]\n",
    "        \n",
    "        return [x + x_distance, y + y_distance]\n",
    "    \n",
    "    def transformLocation(self, x, y):\n",
    "        if not self.base_top_left:\n",
    "            return [64 - x, 64 - y]\n",
    "        \n",
    "        return [x, y]\n",
    "    \n",
    "    def getMeanLocation(self, unitList):\n",
    "        sum_x = 0\n",
    "        sum_y = 0\n",
    "        for unit in unitList:\n",
    "            sum_x += unit.x\n",
    "            sum_y += unit.y\n",
    "        mean_x = sum_x / len(unitList)\n",
    "        mean_y = sum_y / len(unitList)\n",
    "        \n",
    "        return [mean_x, mean_y]\n",
    "    \n",
    "    def splitAction(self, action_id):\n",
    "        smart_action = smart_actions[action_id]\n",
    "            \n",
    "        x = 0\n",
    "        y = 0\n",
    "        if '_' in smart_action:\n",
    "            smart_action, x, y = smart_action.split('_')\n",
    "\n",
    "        return (smart_action, x, y)\n",
    "    \n",
    "    def unit_type_is_selected(self, obs, unit_type):\n",
    "        if (len(obs.observation.single_select) > 0 and\n",
    "            obs.observation.single_select[0].unit_type == unit_type):\n",
    "              return True\n",
    "\n",
    "        if (len(obs.observation.multi_select) > 0 and\n",
    "            obs.observation.multi_select[0].unit_type == unit_type):\n",
    "              return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def get_units_by_type(self, obs, unit_type):\n",
    "        return [unit for unit in obs.observation.feature_units\n",
    "                if unit.unit_type == unit_type]\n",
    "\n",
    "    def can_do(self, obs, action):\n",
    "        return action in obs.observation.available_actions\n",
    "        \n",
    "    def step(self, obs):\n",
    "        super(TerranSparseRewardRLAgent, self).step(obs)\n",
    "\n",
    "        #time.sleep(0.5)\n",
    "        \n",
    "        if obs.first():\n",
    "            player_y, player_x = (obs.observation.feature_minimap.player_relative == features.PlayerRelative.SELF).nonzero()\n",
    "            self.base_top_left = 1 if player_y.any() and player_y.mean() <= 31 else 0\n",
    "        \n",
    "        ccs = self.get_units_by_type(obs, units.Terran.CommandCenter)\n",
    "        if len(ccs) > 0:\n",
    "            self.cc_x, self.cc_y = self.getMeanLocation(ccs)\n",
    "            \n",
    "        cc_count = len(ccs)\n",
    "        \n",
    "        supply_depot_count = len(self.get_units_by_type(obs, units.Terran.SupplyDepot))\n",
    "\n",
    "        barracks_count = len(self.get_units_by_type(obs, units.Terran.Barracks))\n",
    "\n",
    "        army_supply = obs.observation.player.food_used\n",
    "        \n",
    "        if self.move_number == 0:\n",
    "            self.move_number += 1\n",
    "            \n",
    "            current_state = np.zeros(8)\n",
    "            current_state[0] = cc_count\n",
    "            current_state[1] = supply_depot_count\n",
    "            current_state[2] = barracks_count\n",
    "            current_state[3] = army_supply\n",
    "    \n",
    "            hot_squares = np.zeros(4)        \n",
    "            enemy_y, enemy_x = (obs.observation.feature_minimap.player_relative == features.PlayerRelative.ENEMY).nonzero()\n",
    "            for i in range(0, len(enemy_y)):\n",
    "                y = int(math.ceil((enemy_y[i] + 1) / 32))\n",
    "                x = int(math.ceil((enemy_x[i] + 1) / 32))\n",
    "                \n",
    "                hot_squares[((y - 1) * 2) + (x - 1)] = 1\n",
    "            \n",
    "            if not self.base_top_left:\n",
    "                hot_squares = hot_squares[::-1]\n",
    "            \n",
    "            for i in range(0, 4):\n",
    "                current_state[i + 4] = hot_squares[i]\n",
    "    \n",
    "            if self.previous_action is not None:\n",
    "                self.qlearn.learn(str(self.previous_state), self.previous_action, 0, str(current_state))\n",
    "        \n",
    "            rl_action = self.qlearn.choose_action(str(current_state))\n",
    "\n",
    "            self.previous_state = current_state\n",
    "            self.previous_action = rl_action\n",
    "        \n",
    "            smart_action, x, y = self.splitAction(self.previous_action)\n",
    "            \n",
    "            if smart_action == ACTION_BUILD_BARRACKS or smart_action == ACTION_BUILD_SUPPLY_DEPOT:\n",
    "                if self.can_do(obs, actions.FUNCTIONS.select_point.id):\n",
    "                    scvs = self.get_units_by_type(obs, units.Terran.SCV)\n",
    "                    if len(scvs) > 0:\n",
    "                        scv = random.choice(scvs)\n",
    "                        if scv.x >= 0 and scv.y >= 0:\n",
    "                            return actions.FUNCTIONS.select_point(\"select\", (scv.x,\n",
    "                                                                              scv.y))\n",
    "                \n",
    "            elif smart_action == ACTION_BUILD_MARINE:\n",
    "                if self.can_do(obs, actions.FUNCTIONS.select_point.id):\n",
    "                    barracks = self.get_units_by_type(obs, units.Terran.Barracks)\n",
    "                    if len(barracks) > 0:\n",
    "                        barrack = random.choice(barracks)\n",
    "                        if barrack.x >= 0 and barrack.y >= 0:\n",
    "                            return actions.FUNCTIONS.select_point(\"select_all_type\", (barrack.x,\n",
    "                                                                                  barrack.y))\n",
    "                \n",
    "            elif smart_action == ACTION_ATTACK:\n",
    "                if self.can_do(obs, actions.FUNCTIONS.select_army.id):\n",
    "                    return actions.FUNCTIONS.select_army(\"select\")\n",
    "                \n",
    "        elif self.move_number == 1:\n",
    "            self.move_number += 1\n",
    "            \n",
    "            smart_action, x, y = self.splitAction(self.previous_action)\n",
    "                \n",
    "            if smart_action == ACTION_BUILD_SUPPLY_DEPOT:\n",
    "                if supply_depot_count < 2 and self.can_do(obs, actions.FUNCTIONS.Build_SupplyDepot_screen.id):\n",
    "                    if len(ccs) > 0:\n",
    "                        if supply_depot_count == 0:\n",
    "                            target = self.transformDistance(self.cc_x, -35, self.cc_y, 0)\n",
    "                        elif supply_depot_count == 1:\n",
    "                            target = self.transformDistance(self.cc_x, -25, self.cc_y, -25)\n",
    "    \n",
    "                        return actions.FUNCTIONS.Build_SupplyDepot_screen(\"now\", target) \n",
    "            \n",
    "            elif smart_action == ACTION_BUILD_BARRACKS:\n",
    "                if barracks_count < 2 and self.can_do(obs, actions.FUNCTIONS.Build_Barracks_screen.id):\n",
    "                    if len(ccs) > 0:\n",
    "                        if  barracks_count == 0:\n",
    "                            target = self.transformDistance(self.cc_x, 15, self.cc_y, -9)\n",
    "                        elif  barracks_count == 1:\n",
    "                            target = self.transformDistance(self.cc_x, 15, self.cc_y, 12)\n",
    "    \n",
    "                        return actions.FUNCTIONS.Build_Barracks_screen(\"now\", target)\n",
    "    \n",
    "            elif smart_action == ACTION_BUILD_MARINE:\n",
    "                if self.can_do(obs, actions.FUNCTIONS.Train_Marine_quick.id):\n",
    "                    return actions.FUNCTIONS.Train_Marine_quick(\"queued\")\n",
    "        \n",
    "            elif smart_action == ACTION_ATTACK:\n",
    "                if self.can_do(obs, actions.FUNCTIONS.Attack_minimap.id):\n",
    "                    x_offset = random.randint(-1, 1)\n",
    "                    y_offset = random.randint(-1, 1)\n",
    "                    \n",
    "                    return actions.FUNCTIONS.Attack_minimap(\"now\", self.transformLocation(int(x) + (x_offset * 8), int(y) + (y_offset * 8)))\n",
    "            \n",
    "        elif self.move_number == 2:\n",
    "            self.move_number = 0\n",
    "            \n",
    "            smart_action, x, y = self.splitAction(self.previous_action)\n",
    "                \n",
    "            if smart_action == ACTION_BUILD_BARRACKS or smart_action == ACTION_BUILD_SUPPLY_DEPOT:\n",
    "                if self.can_do(obs, actions.FUNCTIONS.Harvest_Gather_screen.id):\n",
    "                    mfs = self.get_units_by_type(obs, units.Neutral.MineralField)\n",
    "                    if len(mfs) > 0:\n",
    "                        mf = random.choice(mfs)\n",
    "                        if mf.x >= 0 and mf.y >= 0:\n",
    "                            return actions.FUNCTIONS.Harvest_Gather_screen(\"now\", (mf.x,mf.y))\n",
    "            \n",
    "        return actions.FUNCTIONS.no_op()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [run code]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "  app.run(main)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Detecting End of Game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import math\n",
    "import os.path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from pysc2.agents import base_agent\n",
    "from pysc2.env import sc2_env\n",
    "from pysc2.lib import actions, features, units\n",
    "from absl import app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FILE = 'rlagent_with_sparse_reward_learning_data'\n",
    "\n",
    "ACTION_DO_NOTHING = 'donothing'\n",
    "ACTION_BUILD_SUPPLY_DEPOT = 'buildsupplydepot'\n",
    "ACTION_BUILD_BARRACKS = 'buildbarracks'\n",
    "ACTION_BUILD_MARINE = 'buildmarine'\n",
    "ACTION_ATTACK = 'attack'\n",
    "\n",
    "smart_actions = [\n",
    "    ACTION_DO_NOTHING,\n",
    "    ACTION_BUILD_SUPPLY_DEPOT,\n",
    "    ACTION_BUILD_BARRACKS,\n",
    "    ACTION_BUILD_MARINE,\n",
    "]\n",
    "\n",
    "for mm_x in range(0, 64):\n",
    "    for mm_y in range(0, 64):\n",
    "        if (mm_x + 1) % 32 == 0 and (mm_y + 1) % 32 == 0:\n",
    "            smart_actions.append(ACTION_ATTACK + '_' + str(mm_x - 16) + '_' + str(mm_y - 16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference from https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow\n",
    "class QLearningTable:\n",
    "    def __init__(self, actions, learning_rate=0.01, reward_decay=0.9, e_greedy=0.9):\n",
    "        self.actions = actions  # a list\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = reward_decay\n",
    "        self.epsilon = e_greedy\n",
    "        self.q_table = pd.DataFrame(columns=self.actions, dtype=np.float64)\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        self.check_state_exist(observation)\n",
    "        \n",
    "        if np.random.uniform() < self.epsilon:\n",
    "            # choose best action\n",
    "            #state_action = self.q_table.ix[observation, :]\n",
    "            state_action = self.q_table.loc[observation, self.q_table.columns[:]]\n",
    "            \n",
    "            # some actions have the same value\n",
    "            state_action = state_action.reindex(np.random.permutation(state_action.index))\n",
    "            \n",
    "            action = state_action.idxmax()\n",
    "        else:\n",
    "            # choose random action\n",
    "            action = np.random.choice(self.actions)\n",
    "            \n",
    "        return action\n",
    "\n",
    "    def learn(self, s, a, r, s_):\n",
    "        self.check_state_exist(s_)\n",
    "        self.check_state_exist(s)\n",
    "        \n",
    "        #q_predict = self.q_table.ix[s, a]\n",
    "        q_predict = self.q_table.loc[s, a]\n",
    "        \n",
    "        if s_ != 'terminal':\n",
    "            #q_target = r + self.gamma * self.q_table.ix[s_, :].max()\n",
    "            q_target = r + self.gamma * self.q_table.loc[s_, self.q_table.columns[:]].max()\n",
    "        else:\n",
    "            q_target = r  # next state is terminal\n",
    "            \n",
    "        # update\n",
    "        #self.q_table.ix[s, a] += self.lr * (q_target - q_predict)\n",
    "        self.q_table.loc[s, a] += self.lr * (q_target - q_predict)\n",
    "\n",
    "    def check_state_exist(self, state):\n",
    "        if state not in self.q_table.index:\n",
    "            # append new state to q table\n",
    "            self.q_table = self.q_table.append(pd.Series([0] * len(self.actions), index=self.q_table.columns, name=state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TerranSparseRewardRLAgent(base_agent.BaseAgent):\n",
    "    def __init__(self):\n",
    "        super(TerranSparseRewardRLAgent, self).__init__()\n",
    "        \n",
    "        self.qlearn = QLearningTable(actions=list(range(len(smart_actions))))\n",
    "        \n",
    "        self.previous_action = None\n",
    "        self.previous_state = None\n",
    "        \n",
    "        self.cc_y = None\n",
    "        self.cc_x = None\n",
    "        \n",
    "        self.move_number = 0\n",
    "        \n",
    "        if os.path.isfile(DATA_FILE + '.gz'):\n",
    "            self.qlearn.q_table = pd.read_pickle(DATA_FILE + '.gz', compression='gzip')\n",
    "\n",
    "    def transformDistance(self, x, x_distance, y, y_distance):\n",
    "        if not self.base_top_left:\n",
    "            return [x - x_distance, y - y_distance]\n",
    "        \n",
    "        return [x + x_distance, y + y_distance]\n",
    "    \n",
    "    def transformLocation(self, x, y):\n",
    "        if not self.base_top_left:\n",
    "            return [64 - x, 64 - y]\n",
    "        \n",
    "        return [x, y]\n",
    "    \n",
    "    def getMeanLocation(self, unitList):\n",
    "        sum_x = 0\n",
    "        sum_y = 0\n",
    "        for unit in unitList:\n",
    "            sum_x += unit.x\n",
    "            sum_y += unit.y\n",
    "        mean_x = sum_x / len(unitList)\n",
    "        mean_y = sum_y / len(unitList)\n",
    "        \n",
    "        return [mean_x, mean_y]\n",
    "    \n",
    "    def splitAction(self, action_id):\n",
    "        smart_action = smart_actions[action_id]\n",
    "            \n",
    "        x = 0\n",
    "        y = 0\n",
    "        if '_' in smart_action:\n",
    "            smart_action, x, y = smart_action.split('_')\n",
    "\n",
    "        return (smart_action, x, y)\n",
    "    \n",
    "    def unit_type_is_selected(self, obs, unit_type):\n",
    "        if (len(obs.observation.single_select) > 0 and\n",
    "            obs.observation.single_select[0].unit_type == unit_type):\n",
    "              return True\n",
    "\n",
    "        if (len(obs.observation.multi_select) > 0 and\n",
    "            obs.observation.multi_select[0].unit_type == unit_type):\n",
    "              return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def get_units_by_type(self, obs, unit_type):\n",
    "        return [unit for unit in obs.observation.feature_units\n",
    "                if unit.unit_type == unit_type]\n",
    "\n",
    "    def can_do(self, obs, action):\n",
    "        return action in obs.observation.available_actions\n",
    "        \n",
    "    def step(self, obs):\n",
    "        super(TerranSparseRewardRLAgent, self).step(obs)\n",
    "\n",
    "        #time.sleep(0.5)\n",
    "        \n",
    "        if obs.last():\n",
    "            reward = obs.reward\n",
    "        \n",
    "            self.qlearn.learn(str(self.previous_state), self.previous_action, reward, 'terminal')\n",
    "            \n",
    "            self.qlearn.q_table.to_pickle(DATA_FILE + '.gz', 'gzip')\n",
    "            \n",
    "            self.previous_action = None\n",
    "            self.previous_state = None\n",
    "            \n",
    "            self.move_number = 0\n",
    "            \n",
    "            return actions.FUNCTIONS.no_op()\n",
    "        \n",
    "        if obs.first():\n",
    "            player_y, player_x = (obs.observation.feature_minimap.player_relative == features.PlayerRelative.SELF).nonzero()\n",
    "            self.base_top_left = 1 if player_y.any() and player_y.mean() <= 31 else 0\n",
    "        \n",
    "        ccs = self.get_units_by_type(obs, units.Terran.CommandCenter)\n",
    "        if len(ccs) > 0:\n",
    "            self.cc_x, self.cc_y = self.getMeanLocation(ccs)\n",
    "            \n",
    "        cc_count = len(ccs)\n",
    "        \n",
    "        supply_depot_count = len(self.get_units_by_type(obs, units.Terran.SupplyDepot))\n",
    "\n",
    "        barracks_count = len(self.get_units_by_type(obs, units.Terran.Barracks))\n",
    "\n",
    "        army_supply = obs.observation.player.food_used\n",
    "        \n",
    "        if self.move_number == 0:\n",
    "            self.move_number += 1\n",
    "            \n",
    "            current_state = np.zeros(8)\n",
    "            current_state[0] = cc_count\n",
    "            current_state[1] = supply_depot_count\n",
    "            current_state[2] = barracks_count\n",
    "            current_state[3] = army_supply\n",
    "    \n",
    "            hot_squares = np.zeros(4)        \n",
    "            enemy_y, enemy_x = (obs.observation.feature_minimap.player_relative == features.PlayerRelative.ENEMY).nonzero()\n",
    "            for i in range(0, len(enemy_y)):\n",
    "                y = int(math.ceil((enemy_y[i] + 1) / 32))\n",
    "                x = int(math.ceil((enemy_x[i] + 1) / 32))\n",
    "                \n",
    "                hot_squares[((y - 1) * 2) + (x - 1)] = 1\n",
    "            \n",
    "            if not self.base_top_left:\n",
    "                hot_squares = hot_squares[::-1]\n",
    "            \n",
    "            for i in range(0, 4):\n",
    "                current_state[i + 4] = hot_squares[i]\n",
    "    \n",
    "            if self.previous_action is not None:\n",
    "                self.qlearn.learn(str(self.previous_state), self.previous_action, 0, str(current_state))\n",
    "        \n",
    "            rl_action = self.qlearn.choose_action(str(current_state))\n",
    "\n",
    "            self.previous_state = current_state\n",
    "            self.previous_action = rl_action\n",
    "        \n",
    "            smart_action, x, y = self.splitAction(self.previous_action)\n",
    "            \n",
    "            if smart_action == ACTION_BUILD_BARRACKS or smart_action == ACTION_BUILD_SUPPLY_DEPOT:\n",
    "                if self.can_do(obs, actions.FUNCTIONS.select_point.id):\n",
    "                    scvs = self.get_units_by_type(obs, units.Terran.SCV)\n",
    "                    if len(scvs) > 0:\n",
    "                        scv = random.choice(scvs)\n",
    "                        if scv.x >= 0 and scv.y >= 0:\n",
    "                            return actions.FUNCTIONS.select_point(\"select\", (scv.x,\n",
    "                                                                              scv.y))\n",
    "                \n",
    "            elif smart_action == ACTION_BUILD_MARINE:\n",
    "                if self.can_do(obs, actions.FUNCTIONS.select_point.id):\n",
    "                    barracks = self.get_units_by_type(obs, units.Terran.Barracks)\n",
    "                    if len(barracks) > 0:\n",
    "                        barrack = random.choice(barracks)\n",
    "                        if barrack.x >= 0 and barrack.y >= 0:\n",
    "                            return actions.FUNCTIONS.select_point(\"select_all_type\", (barrack.x,\n",
    "                                                                                  barrack.y))\n",
    "                \n",
    "            elif smart_action == ACTION_ATTACK:\n",
    "                if self.can_do(obs, actions.FUNCTIONS.select_army.id):\n",
    "                    return actions.FUNCTIONS.select_army(\"select\")\n",
    "                \n",
    "        elif self.move_number == 1:\n",
    "            self.move_number += 1\n",
    "            \n",
    "            smart_action, x, y = self.splitAction(self.previous_action)\n",
    "                \n",
    "            if smart_action == ACTION_BUILD_SUPPLY_DEPOT:\n",
    "                if supply_depot_count < 2 and self.can_do(obs, actions.FUNCTIONS.Build_SupplyDepot_screen.id):\n",
    "                    if len(ccs) > 0:\n",
    "                        if supply_depot_count == 0:\n",
    "                            target = self.transformDistance(self.cc_x, -35, self.cc_y, 0)\n",
    "                        elif supply_depot_count == 1:\n",
    "                            target = self.transformDistance(self.cc_x, -25, self.cc_y, -25)\n",
    "    \n",
    "                        return actions.FUNCTIONS.Build_SupplyDepot_screen(\"now\", target) \n",
    "            \n",
    "            elif smart_action == ACTION_BUILD_BARRACKS:\n",
    "                if barracks_count < 2 and self.can_do(obs, actions.FUNCTIONS.Build_Barracks_screen.id):\n",
    "                    if len(ccs) > 0:\n",
    "                        if  barracks_count == 0:\n",
    "                            target = self.transformDistance(self.cc_x, 15, self.cc_y, -9)\n",
    "                        elif  barracks_count == 1:\n",
    "                            target = self.transformDistance(self.cc_x, 15, self.cc_y, 12)\n",
    "    \n",
    "                        return actions.FUNCTIONS.Build_Barracks_screen(\"now\", target)\n",
    "    \n",
    "            elif smart_action == ACTION_BUILD_MARINE:\n",
    "                if self.can_do(obs, actions.FUNCTIONS.Train_Marine_quick.id):\n",
    "                    return actions.FUNCTIONS.Train_Marine_quick(\"queued\")\n",
    "        \n",
    "            elif smart_action == ACTION_ATTACK:\n",
    "                if self.can_do(obs, actions.FUNCTIONS.Attack_minimap.id):\n",
    "                    x_offset = random.randint(-1, 1)\n",
    "                    y_offset = random.randint(-1, 1)\n",
    "                    \n",
    "                    return actions.FUNCTIONS.Attack_minimap(\"now\", self.transformLocation(int(x) + (x_offset * 8), int(y) + (y_offset * 8)))\n",
    "            \n",
    "        elif self.move_number == 2:\n",
    "            self.move_number = 0\n",
    "            \n",
    "            smart_action, x, y = self.splitAction(self.previous_action)\n",
    "                \n",
    "            if smart_action == ACTION_BUILD_BARRACKS or smart_action == ACTION_BUILD_SUPPLY_DEPOT:\n",
    "                if self.can_do(obs, actions.FUNCTIONS.Harvest_Gather_screen.id):\n",
    "                    mfs = self.get_units_by_type(obs, units.Neutral.MineralField)\n",
    "                    if len(mfs) > 0:\n",
    "                        mf = random.choice(mfs)\n",
    "                        if mf.x >= 0 and mf.y >= 0:\n",
    "                            return actions.FUNCTIONS.Harvest_Gather_screen(\"queued\", (mf.x,mf.y))\n",
    "            \n",
    "        return actions.FUNCTIONS.no_op()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [run code]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "  app.run(main)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Refining \n",
    "- Ignoreing Learing When State Does Not Change\n",
    "- Preventing Invalid Actions\n",
    "- Add Our Unit Locations to the State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import math\n",
    "import os.path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import deque\n",
    "import pickle\n",
    "\n",
    "from pysc2.agents import base_agent\n",
    "from pysc2.env import sc2_env\n",
    "from pysc2.lib import actions, features, units\n",
    "from absl import app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FILE = 'rlagent_with_sparse_reward_learning_data'\n",
    "SCORE_FILE = 'rlagent_with_sparse_reward_learning_score'\n",
    "\n",
    "ACTION_DO_NOTHING = 'donothing'\n",
    "ACTION_BUILD_SUPPLY_DEPOT = 'buildsupplydepot'\n",
    "ACTION_BUILD_BARRACKS = 'buildbarracks'\n",
    "ACTION_BUILD_MARINE = 'buildmarine'\n",
    "ACTION_ATTACK = 'attack'\n",
    "\n",
    "smart_actions = [\n",
    "    ACTION_DO_NOTHING,\n",
    "    ACTION_BUILD_SUPPLY_DEPOT,\n",
    "    ACTION_BUILD_BARRACKS,\n",
    "    ACTION_BUILD_MARINE,\n",
    "]\n",
    "\n",
    "for mm_x in range(0, 64):\n",
    "    for mm_y in range(0, 64):\n",
    "        if (mm_x + 1) % 32 == 0 and (mm_y + 1) % 32 == 0:\n",
    "            smart_actions.append(ACTION_ATTACK + '_' + str(mm_x - 16) + '_' + str(mm_y - 16))\n",
    "            \n",
    "scores = []                        # list containing scores from each episode\n",
    "scores_window = deque(maxlen=100)  # last 100 scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference from https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow\n",
    "class QLearningTable:\n",
    "    def __init__(self, actions, learning_rate=0.01, reward_decay=0.9, e_greedy=0.9):\n",
    "        self.actions = actions  # a list\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = reward_decay\n",
    "        self.epsilon = e_greedy\n",
    "        self.q_table = pd.DataFrame(columns=self.actions, dtype=np.float64)\n",
    "        self.disallowed_actions = {}\n",
    "        \n",
    "    def choose_action(self, observation, excluded_actions=[]):\n",
    "        self.check_state_exist(observation)\n",
    "        \n",
    "        self.disallowed_actions[observation] = excluded_actions\n",
    "        \n",
    "        #state_action = self.q_table.ix[observation, :]\n",
    "        #state_action = self.q_table.loc[observation, self.q_table.columns[:]]\n",
    "        state_action = self.q_table.loc[observation, :]\n",
    "        \n",
    "        for excluded_action in excluded_actions:\n",
    "            del state_action[excluded_action]\n",
    "        \n",
    "        if np.random.uniform() < self.epsilon:            \n",
    "            # some actions have the same value\n",
    "            state_action = state_action.reindex(np.random.permutation(state_action.index))\n",
    "            \n",
    "            action = state_action.idxmax()\n",
    "        else:\n",
    "            # choose random action\n",
    "            action = np.random.choice(state_action.index)\n",
    "            \n",
    "        return action\n",
    "\n",
    "    def learn(self, s, a, r, s_):\n",
    "        if s == s_:\n",
    "            return\n",
    "        \n",
    "        self.check_state_exist(s_)\n",
    "        self.check_state_exist(s)\n",
    "        \n",
    "        #q_predict = self.q_table.ix[s, a]\n",
    "        q_predict = self.q_table.loc[s, a]\n",
    "        \n",
    "        #s_rewards = self.q_table.ix[s_, :]\n",
    "        #s_rewards = self.q_table.loc[s_, self.q_table.columns[:]]\n",
    "        s_rewards = self.q_table.loc[s_, :]\n",
    "        \n",
    "        if s_ in self.disallowed_actions:\n",
    "            for excluded_action in self.disallowed_actions[s_]:\n",
    "                del s_rewards[excluded_action]\n",
    "        \n",
    "        if s_ != 'terminal':\n",
    "            q_target = r + self.gamma * s_rewards.max()\n",
    "        else:\n",
    "            q_target = r  # next state is terminal\n",
    "            \n",
    "        # update\n",
    "        #self.q_table.ix[s, a] += self.lr * (q_target - q_predict)\n",
    "        self.q_table.loc[s, a] += self.lr * (q_target - q_predict)\n",
    "\n",
    "    def check_state_exist(self, state):\n",
    "        if state not in self.q_table.index:\n",
    "            # append new state to q table\n",
    "            self.q_table = self.q_table.append(pd.Series([0] * len(self.actions), index=self.q_table.columns, name=state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TerranSparseRewardRLAgent(base_agent.BaseAgent):\n",
    "    def __init__(self):\n",
    "        super(TerranSparseRewardRLAgent, self).__init__()\n",
    "        \n",
    "        self.qlearn = QLearningTable(actions=list(range(len(smart_actions))))\n",
    "        \n",
    "        self.previous_action = None\n",
    "        self.previous_state = None\n",
    "        \n",
    "        self.cc_y = None\n",
    "        self.cc_x = None\n",
    "        \n",
    "        self.move_number = 0\n",
    "        \n",
    "        if os.path.isfile(DATA_FILE + '.gz'):\n",
    "            self.qlearn.q_table = pd.read_pickle(DATA_FILE + '.gz', compression='gzip')\n",
    "\n",
    "    def transformDistance(self, x, x_distance, y, y_distance):\n",
    "        if not self.base_top_left:\n",
    "            return [x - x_distance, y - y_distance]\n",
    "        \n",
    "        return [x + x_distance, y + y_distance]\n",
    "    \n",
    "    def transformLocation(self, x, y):\n",
    "        if not self.base_top_left:\n",
    "            return [64 - x, 64 - y]\n",
    "        \n",
    "        return [x, y]\n",
    "    \n",
    "    def getMeanLocation(self, unitList):\n",
    "        sum_x = 0\n",
    "        sum_y = 0\n",
    "        for unit in unitList:\n",
    "            sum_x += unit.x\n",
    "            sum_y += unit.y\n",
    "        mean_x = sum_x / len(unitList)\n",
    "        mean_y = sum_y / len(unitList)\n",
    "        \n",
    "        return [mean_x, mean_y]\n",
    "    \n",
    "    def splitAction(self, action_id):\n",
    "        smart_action = smart_actions[action_id]\n",
    "            \n",
    "        x = 0\n",
    "        y = 0\n",
    "        if '_' in smart_action:\n",
    "            smart_action, x, y = smart_action.split('_')\n",
    "\n",
    "        return (smart_action, x, y)\n",
    "    \n",
    "    def unit_type_is_selected(self, obs, unit_type):\n",
    "        if (len(obs.observation.single_select) > 0 and\n",
    "            obs.observation.single_select[0].unit_type == unit_type):\n",
    "              return True\n",
    "\n",
    "        if (len(obs.observation.multi_select) > 0 and\n",
    "            obs.observation.multi_select[0].unit_type == unit_type):\n",
    "              return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def get_units_by_type(self, obs, unit_type):\n",
    "        return [unit for unit in obs.observation.feature_units\n",
    "                if unit.unit_type == unit_type]\n",
    "\n",
    "    def can_do(self, obs, action):\n",
    "        return action in obs.observation.available_actions\n",
    "        \n",
    "    def step(self, obs):\n",
    "        super(TerranSparseRewardRLAgent, self).step(obs)\n",
    "\n",
    "        #time.sleep(0.5)\n",
    "        \n",
    "        if obs.last():\n",
    "            reward = obs.reward\n",
    "        \n",
    "            self.qlearn.learn(str(self.previous_state), self.previous_action, reward, 'terminal')\n",
    "            \n",
    "            self.qlearn.q_table.to_pickle(DATA_FILE + '.gz', 'gzip')\n",
    "            \n",
    "            scores_window.append(obs.reward)  # save most recent reward\n",
    "            win_rate = scores_window.count(1)/len(scores_window)*100\n",
    "            tie_rate = scores_window.count(0)/len(scores_window)*100\n",
    "            lost_rate = scores_window.count(-1)/len(scores_window)*100\n",
    "            \n",
    "            scores.append([win_rate, tie_rate, lost_rate])  # save most recent score(win_rate, tie_rate, lost_rate)\n",
    "            with open(SCORE_FILE + '.txt', \"wb\") as fp:\n",
    "                pickle.dump(scores, fp)\n",
    "            \n",
    "            self.previous_action = None\n",
    "            self.previous_state = None\n",
    "            \n",
    "            self.move_number = 0\n",
    "            \n",
    "            return actions.FUNCTIONS.no_op()\n",
    "        \n",
    "        if obs.first():\n",
    "            player_y, player_x = (obs.observation.feature_minimap.player_relative == features.PlayerRelative.SELF).nonzero()\n",
    "            self.base_top_left = 1 if player_y.any() and player_y.mean() <= 31 else 0\n",
    "        \n",
    "        ccs = self.get_units_by_type(obs, units.Terran.CommandCenter)\n",
    "        if len(ccs) > 0:\n",
    "            self.cc_x, self.cc_y = self.getMeanLocation(ccs)\n",
    "            \n",
    "        cc_count = len(ccs)\n",
    "        \n",
    "        supply_depot_count = len(self.get_units_by_type(obs, units.Terran.SupplyDepot))\n",
    "\n",
    "        barracks_count = len(self.get_units_by_type(obs, units.Terran.Barracks))\n",
    "        \n",
    "        supply_used = obs.observation.player.food_used\n",
    "        supply_limit = obs.observation.player.food_cap\n",
    "        army_supply = obs.observation.player.food_army\n",
    "        worker_supply = obs.observation.player.food_workers\n",
    "        \n",
    "        supply_free = supply_limit - supply_used\n",
    "        \n",
    "        if self.move_number == 0:\n",
    "            self.move_number += 1\n",
    "            \n",
    "            \n",
    "            current_state = np.zeros(12)\n",
    "            current_state[0] = cc_count\n",
    "            current_state[1] = supply_depot_count\n",
    "            current_state[2] = barracks_count\n",
    "            current_state[3] = army_supply\n",
    "    \n",
    "            hot_squares = np.zeros(4)        \n",
    "            enemy_y, enemy_x = (obs.observation.feature_minimap.player_relative == features.PlayerRelative.ENEMY).nonzero()\n",
    "            for i in range(0, len(enemy_y)):\n",
    "                y = int(math.ceil((enemy_y[i] + 1) / 32))\n",
    "                x = int(math.ceil((enemy_x[i] + 1) / 32))\n",
    "                \n",
    "                hot_squares[((y - 1) * 2) + (x - 1)] = 1\n",
    "            \n",
    "            if not self.base_top_left:\n",
    "                hot_squares = hot_squares[::-1]\n",
    "            \n",
    "            for i in range(0, 4):\n",
    "                current_state[i + 4] = hot_squares[i]\n",
    "                \n",
    "            green_squares = np.zeros(4)        \n",
    "            friendly_y, friendly_x = (obs.observation.feature_minimap.player_relative == features.PlayerRelative.SELF).nonzero()\n",
    "            for i in range(0, len(friendly_y)):\n",
    "                y = int(math.ceil((friendly_y[i] + 1) / 32))\n",
    "                x = int(math.ceil((friendly_x[i] + 1) / 32))\n",
    "                \n",
    "                green_squares[((y - 1) * 2) + (x - 1)] = 1\n",
    "            \n",
    "            if not self.base_top_left:\n",
    "                green_squares = green_squares[::-1]\n",
    "            \n",
    "            for i in range(0, 4):\n",
    "                current_state[i + 8] = green_squares[i]\n",
    "    \n",
    "            if self.previous_action is not None:\n",
    "                self.qlearn.learn(str(self.previous_state), self.previous_action, 0, str(current_state))\n",
    "        \n",
    "            excluded_actions = []\n",
    "            if supply_depot_count == 2 or worker_supply == 0:\n",
    "                excluded_actions.append(1)\n",
    "                \n",
    "            if supply_depot_count == 0 or barracks_count == 2 or worker_supply == 0:\n",
    "                excluded_actions.append(2)\n",
    "\n",
    "            if supply_free == 0 or barracks_count == 0:\n",
    "                excluded_actions.append(3)\n",
    "                \n",
    "            if army_supply == 0:\n",
    "                excluded_actions.append(4)\n",
    "                excluded_actions.append(5)\n",
    "                excluded_actions.append(6)\n",
    "                excluded_actions.append(7)\n",
    "        \n",
    "            rl_action = self.qlearn.choose_action(str(current_state), excluded_actions)\n",
    "\n",
    "            self.previous_state = current_state\n",
    "            self.previous_action = rl_action\n",
    "        \n",
    "            smart_action, x, y = self.splitAction(self.previous_action)\n",
    "            \n",
    "            if smart_action == ACTION_BUILD_BARRACKS or smart_action == ACTION_BUILD_SUPPLY_DEPOT:\n",
    "                if self.can_do(obs, actions.FUNCTIONS.select_point.id):\n",
    "                    scvs = self.get_units_by_type(obs, units.Terran.SCV)\n",
    "                    if len(scvs) > 0:\n",
    "                        scv = random.choice(scvs)\n",
    "                        if scv.x >= 0 and scv.y >= 0:\n",
    "                            return actions.FUNCTIONS.select_point(\"select\", (scv.x,\n",
    "                                                                              scv.y))\n",
    "                \n",
    "            elif smart_action == ACTION_BUILD_MARINE:\n",
    "                if self.can_do(obs, actions.FUNCTIONS.select_point.id):\n",
    "                    barracks = self.get_units_by_type(obs, units.Terran.Barracks)\n",
    "                    if len(barracks) > 0:\n",
    "                        barrack = random.choice(barracks)\n",
    "                        if barrack.x >= 0 and barrack.y >= 0:\n",
    "                            return actions.FUNCTIONS.select_point(\"select_all_type\", (barrack.x,\n",
    "                                                                                  barrack.y))\n",
    "                \n",
    "            elif smart_action == ACTION_ATTACK:\n",
    "                if self.can_do(obs, actions.FUNCTIONS.select_army.id):\n",
    "                    return actions.FUNCTIONS.select_army(\"select\")\n",
    "                \n",
    "        elif self.move_number == 1:\n",
    "            self.move_number += 1\n",
    "            \n",
    "            smart_action, x, y = self.splitAction(self.previous_action)\n",
    "                \n",
    "            if smart_action == ACTION_BUILD_SUPPLY_DEPOT:\n",
    "                if supply_depot_count < 2 and self.can_do(obs, actions.FUNCTIONS.Build_SupplyDepot_screen.id):\n",
    "                    if len(ccs) > 0:\n",
    "                        if supply_depot_count == 0:\n",
    "                            target = self.transformDistance(self.cc_x, -35, self.cc_y, 0)\n",
    "                        elif supply_depot_count == 1:\n",
    "                            target = self.transformDistance(self.cc_x, -25, self.cc_y, -25)\n",
    "    \n",
    "                        return actions.FUNCTIONS.Build_SupplyDepot_screen(\"now\", target) \n",
    "            \n",
    "            elif smart_action == ACTION_BUILD_BARRACKS:\n",
    "                if barracks_count < 2 and self.can_do(obs, actions.FUNCTIONS.Build_Barracks_screen.id):\n",
    "                    if len(ccs) > 0:\n",
    "                        if  barracks_count == 0:\n",
    "                            target = self.transformDistance(self.cc_x, 15, self.cc_y, -9)\n",
    "                        elif  barracks_count == 1:\n",
    "                            target = self.transformDistance(self.cc_x, 15, self.cc_y, 12)\n",
    "    \n",
    "                        return actions.FUNCTIONS.Build_Barracks_screen(\"now\", target)\n",
    "    \n",
    "            elif smart_action == ACTION_BUILD_MARINE:\n",
    "                if self.can_do(obs, actions.FUNCTIONS.Train_Marine_quick.id):\n",
    "                    return actions.FUNCTIONS.Train_Marine_quick(\"queued\")\n",
    "        \n",
    "            elif smart_action == ACTION_ATTACK:\n",
    "                if self.can_do(obs, actions.FUNCTIONS.Attack_minimap.id):\n",
    "                    x_offset = random.randint(-1, 1)\n",
    "                    y_offset = random.randint(-1, 1)\n",
    "                    \n",
    "                    return actions.FUNCTIONS.Attack_minimap(\"now\", self.transformLocation(int(x) + (x_offset * 8), int(y) + (y_offset * 8)))\n",
    "            \n",
    "        elif self.move_number == 2:\n",
    "            self.move_number = 0\n",
    "            \n",
    "            smart_action, x, y = self.splitAction(self.previous_action)\n",
    "                \n",
    "            if smart_action == ACTION_BUILD_BARRACKS or smart_action == ACTION_BUILD_SUPPLY_DEPOT:\n",
    "                if self.can_do(obs, actions.FUNCTIONS.Harvest_Gather_screen.id):\n",
    "                    mfs = self.get_units_by_type(obs, units.Neutral.MineralField)\n",
    "                    if len(mfs) > 0:\n",
    "                        mf = random.choice(mfs)\n",
    "                        if mf.x >= 0 and mf.y >= 0:\n",
    "                            return actions.FUNCTIONS.Harvest_Gather_screen(\"queued\", (mf.x,mf.y))\n",
    "            \n",
    "        return actions.FUNCTIONS.no_op()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [run code]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "  app.run(main)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Winning rate graph]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(SCORE_FILE + '.txt', \"rb\") as fp:\n",
    "    scores = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_scores = np.array(scores)\n",
    "np_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(np_scores)), np_scores.T[0], color='r', label='win rate')\n",
    "plt.plot(np.arange(len(np_scores)), np_scores.T[1], color='g', label='tie rate')\n",
    "plt.plot(np.arange(len(np_scores)), np_scores.T[2], color='b', label='lose rate')\n",
    "plt.ylabel('Score %')\n",
    "plt.xlabel('Episode #')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "starcraft2",
   "language": "python",
   "name": "starcraft2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
